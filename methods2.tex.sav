Regardless of the temporal mining task, the first step of an algorithm is to transform,
or abstract, the raw data into a more concise representation, preserving as much of the
information contained in the original sequence as possible. Semi-parametric temporal
clustering makes useful parametric assumptions for approximate temporal data and
regularizes sequences for input to a clustering step. The temporal abstractions, abstractions
of the raw time-series, function as input to a non-parametric clustering algorithm seeking to more naturally reveal underlying structure.

\subsection{CT-DBN Abstraction}
DBNs require that continuous time is represented as a series of uniform  length units equal to the smallest time granularity in any observed sequence. When data is missing or otherwise incomplete, information is still propagated throughout the model for every time step. It has been shown that when data is not missing at random, which often the case in clinical data, it can result in significantly more EM steps to learn
the model parameters, and more importantly, lead to erroneous findings.

CT-DBNs avoid this limitation by enabling a more natural temporal representation that is more appropriate for modelling dynamic changes that progress at different rates, and non-liner time~\cite{Nodelman02}. By default, all traditional HMMs, and dynamic Bayesian networks more generally, make discrete-time assumptions. A main distinguishing characteristic between DT and CT markov models is that in the discrete-time case, the Markov process stays in a state $i$ for a time distributed according to $F_{i}(t)$ and in the continuous-time case the holding time is \emph{exponentially} distributed according to $F_{i}(t) = e^{q_{i}t}$ where $q_{i}$ is the intensity of the transitions, or the tendency to change state.

\subsubsection{Model Description}
The specific class of CT-DBNs we use for temporal abstraction are based on MSMs that are discussed in more detail in Section~\ref{prelims}.  MSMs applications model disease dynamics at the population level, and can provide information about the influence of a model variable to make predictions about a patients disease related risk.  However, the increased availability of temporal data allows us to extend these models for creating patient specific instead of a population level models, that can be used to make clinical comparisons at the individual and the group level.

Similar to a dynamic Bayesian network, arrows represent the allowable transition between model states.  The main distinctions are the application specification that governs their design and interpretation.  A 4 state MSM is shown in Figure \ref{fig:msm}, with the intensity matrix $Q$ that represents the instantaneous behavior of the process $X$.  Similar to the extension of Markov models for latent variables, an additional emission matrix is required for the model definition, with matrix entries indicating the probability of each of the $m$ observations conditioned on each model state.

 In contrast to a discrete-time transition matrix for a process variable $X$ with a domain of of ${x_1,x_2,...,x_n}$ where $n$ corresponds with the number of states, the intuition is that the intensity, $q_{i}$, no longer corresponds with the transition probability that is constant for the length of a time slice, but rather an `\emph{instantaneous probability}' of leaving state $x_i$ and the intensity of $q_{i,j}$ gives the `instantaneous probability' of transitioning from $x_i$ to $x_j$.

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,semithick]
\tikzstyle{every state}=[fill=blue!8,draw=black,thick,text=black,scale=1]


\node[state]         (A)              {$q_{1}$};
\node[state]         (B) [right of=A] {$q_{2}$};
\node[state]         (C) [below of=A] {$q_{3}$};
\node[state]         (D) [below of=B] {$q_{4}$};


\path (A.10) edge  [right] node[right] {} (B.170);
\path (A.260) edge  [right] node[below] {} (C.100);
\path (A.320) edge  [right] node[below] {} (D.120);
\path (B.190) edge  [right] node[left] {} (A.350);
\path (B.215) edge  [right] node[below] {} (C.55);
\path (B.260) edge  [right] node[below] {} (D.100);
\path (C.40) edge  [right] node[above] {} (B.230);
\path (C.80) edge  [right] node[above] {} (A.280);
\path (C.10) edge  [right] node[right] {} (D.170);
\path (D.190) edge  [right] node[left] {} (C.350);
\path (D.135) edge  [right] node[above] {} (A.305);
\path (D.80) edge  [right] node[above] {} (B.280);
\end{tikzpicture}
\[
Q_X =
  \begin{bmatrix}
    -q_{1,1} & q_{1,2} & q_{1,3} & q_{1,4}\\
    q_{2,1} & -q_{2,2} & q_{2,3} & q_{2,4}\\
    q_{3,1} & q_{3,2} & -q_{3,3} & q_{3,4}\\
    q_{4,1} & q_{4,2} & q_{4,3} & -q_{4,4}\\
  \end{bmatrix}
\]
\end{center}
\caption{Multi-state Markov model with four discrete states that correspond with disease severity or risk, and the intensity matrix}
\label{fig:msm}
\end{figure}

In contrast to discrete-time BNs, the transitions model has an exponential distribution between transition states.  Also, the rows in the matrix $Q$ sum to zero instead of one, with the sum of all transition intensities $q_{r,s}$ in the $r$th row, where $s\neq r$, equal to the absolute value of  $q_{r,r}$ 

$$q_{r,r} = -\sum_{s\neq r} q_{r,s}$$

\noindent and the probability of observing $s$ immediately after state $r$ is $q_{r,s}/q_{r,r}$.

At a time $t$ a patient is in the $S(t)$ state.  For each pair of states, where $z(t)$ is a model variable, the set of transition intensities $q_{r,s}(t,z(t))$ is dependent on $t$ and the instantaneous risk of transitioning from state $r$ to state $s$

$$q_{r,s}(t,z(t)) =  \lim_{\delta t \to +0} P(S(t+\delta t ) = s | S(t) = r) / \delta t $$

\subsubsection{Model Estimation and Parameter Learning}
Ideally, expert knowledge is available to determine the number of disease states and define the initial probabilities for the matrices.  MSMs defined by experts tend have to have few states, and the simplest are known as two-state life and death models, and many have three states corresponding with low, normal and high.  

In the case where the number of states and unknown, a nonparametric density estimation technique can be used to discover the model states, and the MSM framework provided the guidelines for structure that requires their them to be ordered in increasing or increasing order.  When number of states is not available, we use a Dirichlet process mixture model for nonparametric density estimation.  To select appropriate initialization values for the state and emission probability matrices, we evaluate candidate combinations and average the model likelihood.

For discrete models, the Expectation-Maximization algorithm~\cite{Dempster77} is used to estimate the model parameters using a forward-backward method, and extensions exist for the continuous-time setting.  To learn the model parameters in our work we use the BFGS quasi-Newton optimization algorithm for up to 1000 itterations.

\subsection{Nonparametric Bayesian Clustering}
A main challenge posed by many traditional clustering algorithms is selecting the number of groups, $k$.   Some approaches uses to estimate $k$ are based on the spectral gap, or predictive estimates.  However, these are heuristics, and don't guarantee the choice of $k$, and more importantly, for many clustering problems $k$ is unrealistic to assume that $k$ is fixed.

By defining the clustering problem as identifying the components of infinite mixture, where $k$ is random variable in the model,  nonparametric Bayesian approaches allow for the definition of more flexible clustering models.  Relative to other nonparametric clustering methods such as spectral clustering, they do not require that $k$ is expressed a priori.  Also, nonparametric Bayesian cluttering provides a generative model that can describe group structure at the population and subpopulations level, more easily lending itself to interpretation by a domain expert.

\subsubsection{Mixture Models}
Nonparametric Bayesian methods can be used to identify the number of component, and their densities, in a finite mixture model.  The density function of a finite mixture model is defined as:

$$p(x) = \displaystyle\sum\limits_{k=1}^K \pi_k p(x|\theta_k)$$

where $x$ is the data set, $\pi$ is the mixing proportion, and $\theta_k$ are the model parameters for the cluster $k$.

In the nonparametric Bayesian application setting, we define the mixture model as that of one with infinite components.  We can define the discrete case in the form of the integral $p(x) = \int p(x|\theta)G(\theta)d\theta$, where $G = \Sigma_{k=1}^K \pi_k \delta_{\theta_k}$~\cite{OrbanzT10}, which extends to the following in the case of infinite components:

  $$G = \displaystyle\sum_{k=1}^\infty \pi_k \delta_{\theta_k}$$

for a Bayesian non-parametric models with a potentially infinite value of $k$.

\subsubsection{Dirichlet Distribution}
In Bayesian statistics, a \emph{Dirichlet distribution}, Dir$(\alpha)$, is the conjugate prior of the categorical distribution and multinomial distribution.   In terms of a finite mixture model with $k$ components, it is a $k$-dimensional generalization of the beta distribution.  Since we can view model components as groups in the data, it has natural extensions for clustering.   

Specifically, if a population can be described by the probability distribution $\Theta$ with components $\theta_{1},...,\theta_{k}$ that sum to 1, we can reasonably infer
$$\Theta \sim \text{Dirichlet}\{\alpha_{1},...,\alpha_{k}\}$$

The probability density function for a Dirichlet distribution uses a normalization factor that is defined in terms of the \emph{multinomial beta function}, $B(\alpha)$,  that is expressed in terms of the gamma function:

  $$B(\alpha)= \frac{\prod_{i=1}^{k} \Gamma(\alpha_{i})} {\Gamma(\sum_{i=1}^{k} \alpha_{i})}$$

\noindent and the probabilities $p$ and parameters $\alpha$ of each of the $k$ components are~\cite{Ferguson1973}:

$$\text{Dirichlet}(p;\alpha)=\frac{1}{B(\alpha)}\prod_{i=1}^{k} p_{i}^{\alpha_{i}-1}$$


\subsubsection{Dirichlet Process Mixture Models}
One approach to nonparametric Bayesian clustering is \emph{Dirichlet process Gaussian mixture modeling} (DPGMM). Here, a Dirichlet process (DP) is the prior over the mixing distribution, $G$, and defines a measure on measures. It is characterized by two parameters: a base distribution $G_0$, from which samples are drawn, and is the parameter on which the nonparametric distribution is centered.  The second is a positive scaling parameter $\alpha$.  More intuitively described as a `splitting' criteria, $\alpha$ is a scaling factor that is associated with the probability of forming a new cluster.  The base distribution is defined by $G_0$.

$$G \sim DP(G_0,\alpha)$$

For a sample, $G$, drawn from the base distribution $G_0$, if $G \sim DP(G_0,\alpha)$ then for any set of partitions $A_1 \cup A_2 \cup ... A_k$ of $A$:
$$(G(A_1),...,G(A_k)) \sim Dir(\alpha G_0(A_1),...,\alpha G_0(A_k))$$

In the the Dirichlet process mixture model, the DP is used as nonparametric prior in a hierarchical Bayesian model, where $G$ is portioned according to the prior.  These priors are often selected for mathematical convenience.  DPMMs were first applied for density estimation, but is now applied widely for clustering~\cite{Gorur}.   Dirichlet Process Gaussian Mixture Modeling (DPGMM) defines a  Dirichlet process mixture model by taking the limit of the number of mixture components as a hierarchical Gaussian mixture model approaches infinity.  Two methods used to specify the priors are Markov cain Monte-Carlo (MCMC) and variational inference, the later of which is described in the literature~\cite{Blei04} and implemented for out clustering step.

\subsection{Software}
Two open source packages for our task, and adapted functions to our specific needs.  Python's sci-kit learn~\cite{scikit} provides an implementation for Dirichlet process Gaussian mixture modeling, and several clustering validation measures that were used for this work.  For temporal abstraction we modified the MSM package~\cite{Jackson10} to fit general continuous-time Markov and hidden Markov multi-state models to model individuals.  Although we cannot make our data sets available to the public, we plan to release our implementation code and a dummy example that calls the Python code from R, and uses R's ggplot2~\cite{ggplot2} plotting library to create some of the visualizations seen in this paper.  